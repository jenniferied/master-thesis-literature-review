% Domain 8b: Audio Generation for Games
% December 2025

% === MUSIC GENERATION ===

@article{copet2023musicgen,
  title={{MusicGen}: Simple and Controllable Music Generation},
  author={Copet, Jade and Kreuk, Felix and Gat, Itai and Remez, Tal and Kant, David and Synnaeve, Gabriel and Adi, Yossi and Défossez, Alexandre},
  journal={arXiv preprint arXiv:2306.05284},
  year={2023},
  note={~400 citations - Text-to-music, part of AudioCraft}
}

@article{agostinelli2023musiclm,
  title={{MusicLM}: Generating Music From Text},
  author={Agostinelli, Andrea and Denk, Timo I. and Borsos, Zalán and Engel, Jesse and Verzetti, Mauro and Caillon, Antoine and Huang, Qingqing and Jansen, Aren and Roberts, Adam and Tagliasacchi, Marco and others},
  journal={arXiv preprint arXiv:2301.11325},
  year={2023},
  note={~500 citations - Hierarchical music generation}
}

@article{huang2023noise2music,
  title={{Noise2Music}: Text-conditioned Music Generation with Diffusion Models},
  author={Huang, Qingqing and Park, Daniel S. and Wang, Tao and Denk, Timo I. and Ly, Andy and Chen, Nanxin and Zhang, Zhengdong and Zhang, Zhishuai and Yu, Jiahui and Frank, Christian and others},
  journal={arXiv preprint arXiv:2302.03917},
  year={2023},
  note={~200 citations - Diffusion for music}
}

% === VOICE SYNTHESIS ===

@misc{nvidia2024audio2face,
  title={{Audio2Face}: {AI}-Powered Facial Animation},
  author={{NVIDIA}},
  year={2024},
  url={https://developer.nvidia.com/omniverse/audio2face},
  note={Industry tool - Open source speech-to-face animation}
}

@misc{bark2023,
  title={{Bark}: Text-to-Audio Model},
  author={{Suno AI}},
  year={2023},
  url={https://github.com/suno-ai/bark},
  note={Open source - Expressive TTS with emotions}
}

@article{casanova2024xtts,
  title={{XTTS}: A Massively Multilingual Zero-Shot Text-to-Speech Model},
  author={Casanova, Edresson and Davis, Kelly and Gölge, Eren and Göknar, Görkem and Guner, Iulian and Oliveira, Josman and Ogun, Atila and Weber, Julian and others},
  journal={arXiv preprint},
  year={2024},
  note={Open source - 17-language voice cloning}
}

% === SOUND EFFECTS ===

@inproceedings{liu2023audioldm,
  title={{AudioLDM}: Text-to-Audio Generation with Latent Diffusion Models},
  author={Liu, Haohe and Chen, Zehua and Yuan, Yi and Mei, Xinhao and Liu, Xubo and Mandic, Danilo and Wang, Wenwu and Plumbley, Mark D.},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2023},
  note={~800 citations - Foundational text-to-SFX}
}

@inproceedings{liu2024audioldm2,
  title={{AudioLDM 2}: Learning Holistic Audio Generation with Self-supervised Pretraining},
  author={Liu, Haohe and Yuan, Yi and Liu, Xubo and Mei, Xinhao and Kong, Qiuqiang and Tian, Qiao and Wang, Yuping and Wang, Wenwu and Wang, Yuxuan and Plumbley, Mark D.},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2024},
  note={~300 citations - Unified speech/music/SFX}
}

@misc{meta2023audiogen,
  title={{AudioGen}: Textually-Guided Audio Generation},
  author={{Meta AI}},
  year={2023},
  url={https://ai.meta.com/resources/models-and-libraries/audiocraft/},
  note={Part of AudioCraft - Environmental sound effects}
}

% === VOICE-TO-ANIMATION ===

@inproceedings{richard2021meshtalk,
  title={{MeshTalk}: {3D} Face Animation from Speech using Cross-Modality Disentanglement},
  author={Richard, Alexander and Zollhöfer, Michael and Wen, Yandong and de la Torre, Fernando and Sheikh, Yaser},
  booktitle={IEEE/CVF International Conference on Computer Vision (ICCV)},
  pages={1173--1182},
  year={2021},
  note={~200 citations - Direct mesh animation from audio}
}
