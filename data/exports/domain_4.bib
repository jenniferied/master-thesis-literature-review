% Domain 4: 3D Generation & Neural Rendering
% Citation counts from Semantic Scholar/Web Search, December 2025
% NOTE: Some counts estimated. Verify with Google Scholar when rate limit resets

% === TIER 1: Mega-Foundational (>5,000 citations) ===

@inproceedings{mildenhall2020nerf,
  title={{NeRF}: Representing Scenes as Neural Radiance Fields for View Synthesis},
  author={Mildenhall, Ben and Srinivasan, Pratul P. and Tancik, Matthew and Barron, Jonathan T. and Ramamoorthi, Ravi and Ng, Ren},
  booktitle={European Conference on Computer Vision (ECCV)},
  pages={405--421},
  year={2020},
  organization={Springer},
  url={https://arxiv.org/abs/2003.08934},
  note={7,013 citations - Foundational neural radiance field paper, Best Paper Honorable Mention}
}

@article{kerbl2023gaussian,
  title={{3D} Gaussian Splatting for Real-Time Radiance Field Rendering},
  author={Kerbl, Bernhard and Kopanas, Georgios and Leimk{\"u}hler, Thomas and Drettakis, George},
  journal={ACM Transactions on Graphics},
  volume={42},
  number={4},
  pages={1--14},
  year={2023},
  publisher={ACM},
  doi={10.1145/3592433},
  note={~5,599 citations - Real-time neural rendering breakthrough}
}

% === TIER 2: Major Foundational (1,000-5,000 citations) ===

@inproceedings{poole2023dreamfusion,
  title={{DreamFusion}: Text-to-{3D} using {2D} Diffusion},
  author={Poole, Ben and Jain, Ajay and Barron, Jonathan T. and Mildenhall, Ben},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2023},
  url={https://arxiv.org/abs/2209.14988},
  note={2,962 citations - Score Distillation Sampling for text-to-3D}
}

@article{muller2022instant,
  title={Instant Neural Graphics Primitives with a Multiresolution Hash Encoding},
  author={M{\"u}ller, Thomas and Evans, Alex and Schied, Christoph and Keller, Alexander},
  journal={ACM Transactions on Graphics},
  volume={41},
  number={4},
  pages={1--15},
  year={2022},
  publisher={ACM},
  doi={10.1145/3528223.3530127},
  note={~2,500 citations - SIGGRAPH 2022 Best Paper, training in seconds}
}

@inproceedings{lin2023magic3d,
  title={{Magic3D}: High-Resolution Text-to-{3D} Content Creation},
  author={Lin, Chen-Hsuan and Gao, Jun and Tang, Luming and Takikawa, Towaki and Zeng, Xiaohui and Huang, Xun and Kreis, Karsten and Fidler, Sanja and Liu, Ming-Yu and Lin, Tsung-Yi},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={300--309},
  year={2023},
  note={~1,500 citations - CVPR Highlight, 8x higher resolution than DreamFusion}
}

% === TIER 3: Field-Defining (100-1,000 citations) ===

@inproceedings{gao2022get3d,
  title={{GET3D}: A Generative Model of High Quality {3D} Textured Shapes Learned from Images},
  author={Gao, Jun and Shen, Tianchang and Wang, Zian and Chen, Wenzheng and Yin, Kangxue and Li, Daiqing and Litany, Or and Gojcic, Zan and Fidler, Sanja},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  volume={35},
  year={2022},
  url={https://arxiv.org/abs/2209.11163},
  note={~800 citations - Generative textured meshes from images}
}

@article{nichol2022pointe,
  title={{Point-E}: A System for Generating {3D} Point Clouds from Complex Prompts},
  author={Nichol, Alex and Jun, Heewoo and Dhariwal, Prafulla and Mishkin, Pamela and Chen, Mark},
  journal={arXiv preprint arXiv:2212.08751},
  year={2022},
  url={https://arxiv.org/abs/2212.08751},
  note={~600 citations - Fast text-to-3D point clouds in minutes}
}

@article{jun2023shape,
  title={{Shap-E}: Generating Conditional {3D} Implicit Functions},
  author={Jun, Heewoo and Nichol, Alex},
  journal={arXiv preprint arXiv:2305.02463},
  year={2023},
  url={https://arxiv.org/abs/2305.02463},
  note={~500 citations - Improved Point-E with implicit function output}
}
